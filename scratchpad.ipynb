{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIYdn1woOS1n",
    "outputId": "fdfa7e45-7cbb-4035-aec3-a72d4231b597"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive' , force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xqPPfyaqvwBE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qfIZB4R1vxwm",
    "outputId": "57988039-95c1-48df-e306-9e74a74cf35e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replace WESAD/S10/S10.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip -q \"/content/drive/MyDrive/WESAD (1).zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAkheiOl9j-E"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax,Nadam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, Activation, Input, ZeroPadding2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.regularizers import l1, l2\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuPjDXV7-gN5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(\n",
    "    '/content/WESAD/'\n",
    ")\n",
    "for file in files:\n",
    "  if file.endswith('.pdf') : continue\n",
    "  for d in os.listdir(f'/content/WESAD/{file}'):\n",
    "    if d.endswith('.csv'):\n",
    "      data = pd.read_csv(f'/content/WESAD/{file}/{d}')\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aopioOZmGVGf"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Whz_LxC3nR2p"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "base_path = '/content/WESAD/'\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "\n",
    "subject_folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
    "\n",
    "for subject in subject_folders:\n",
    "    try:\n",
    "        subject_path = os.path.join(base_path, subject)\n",
    "\n",
    "        with open(os.path.join(subject_path, f\"{subject}.pkl\"), 'rb') as f:\n",
    "            data = pickle.load(f, encoding='latin1')  \n",
    "\n",
    "        \n",
    "        chest_acc = data['signal']['chest']['ACC']  \n",
    "        chest_eda = data['signal']['chest']['EDA']  \n",
    "        chest_temp = data['signal']['chest']['TEMP']  \n",
    "\n",
    "        wrist_acc = data['signal']['wrist']['ACC']  \n",
    "        wrist_eda = data['signal']['wrist']['EDA']  \n",
    "        wrist_temp = data['signal']['wrist']['TEMP']  \n",
    "\n",
    "        \n",
    "        combined_features = np.hstack((\n",
    "            chest_acc, chest_eda[:, np.newaxis], chest_temp[:, np.newaxis],\n",
    "            wrist_acc, wrist_eda[:, np.newaxis], wrist_temp[:, np.newaxis]\n",
    "        ))\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        standardized_features = scaler.fit_transform(combined_features)\n",
    "\n",
    "        \n",
    "        labels = data['label']  \n",
    "\n",
    "        X_data.append(standardized_features)\n",
    "        y_data.append(labels)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject {subject}: {e}\")\n",
    "\n",
    "if X_data and y_data:\n",
    "    \n",
    "    X_data = np.concatenate(X_data, axis=0)\n",
    "    y_data = np.concatenate(y_data, axis=0)\n",
    "\n",
    "    \n",
    "    X_data_padded = pad_sequences(X_data, padding='post', dtype='float32')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_data_padded, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"y_train shape:\", y_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "else:\n",
    "    print(\"No valid data was processed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4642DrmYpbZP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_path = '/content/WESAD/S15/S15_quest.csv'\n",
    "\n",
    "data = pd.read_csv(file_path, header=None, delimiter=';')\n",
    "\n",
    "data = data[~data[0].astype(str).str.startswith('#')]\n",
    "data = data.dropna(how='all')  \n",
    "\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"Cleaned Data Sample:\")\n",
    "print(data.head())\n",
    "\n",
    "sections = {\n",
    "    'PANAS': data[data[0].str.contains('PANAS', na=False)].iloc[:, 1:].astype(float).values,\n",
    "    'STAI': data[data[0].str.contains('STAI', na=False)].iloc[:, 1:].astype(float).values,\n",
    "    'DIM': data[data[0].str.contains('DIM', na=False)].iloc[:, 1:].astype(float).values,\n",
    "    'SSSQ': data[data[0].str.contains('SSSQ', na=False)].iloc[:, 1:].astype(float).values,\n",
    "}\n",
    "\n",
    "X_data = np.hstack([sections['PANAS'], sections['STAI'], sections['DIM'], sections['SSSQ']])\n",
    "\n",
    "SSSQ_mean = np.mean(sections['SSSQ'], axis=1)\n",
    "y_data = np.where(SSSQ_mean > 3, 1, 0)  \n",
    "\n",
    "print(f\"Feature shape: {X_data.shape}, Label shape: {y_data.shape}\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(X_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbyBhxWtpkVE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test_rnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "def create_rnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(64, input_shape=input_shape, activation='tanh', return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(SimpleRNN(32, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (X_train_rnn.shape[1], X_train_rnn.shape[2])\n",
    "model = create_rnn_model(input_shape)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "history = model.fit(X_train_rnn, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_rnn, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "y_pred = model.predict(X_test_rnn)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsSQ9dRlouRj"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test_rnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "def create_rnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(64, input_shape=input_shape, activation='tanh', return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(SimpleRNN(32, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))  \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (X_train_rnn.shape[1], X_train_rnn.shape[2])\n",
    "model = create_rnn_model(input_shape)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train_rnn, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_rnn, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "y_pred = model.predict(X_test_rnn)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGV_GLpdKU63"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "processed_data = pd.DataFrame()\n",
    "\n",
    "files = os.listdir('/content/WESAD/')\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('.pdf'):  \n",
    "        continue\n",
    "\n",
    "    for d in os.listdir(f'/content/WESAD/{file}'):\n",
    "        if d.endswith('.csv'): \n",
    "            \n",
    "            try:\n",
    "                data = pd.read_csv(f'/content/WESAD/{file}/{d}', header=None, delimiter=';')\n",
    "\n",
    "                \n",
    "                print(f\"Loaded {d} from {file}\")\n",
    "                print(data.head())  \n",
    "\n",
    "                data = data[~data.apply(lambda row: row.astype(str).str.contains(';;;;', regex=False).all(), axis=1)]\n",
    "\n",
    "                # Step 2: Remove rows that start with '#', as they are metadata or comments\n",
    "                data = data[~data[0].str.startswith('#', na=False)]\n",
    "\n",
    "                # Step 3: Reset the columns if any rows are removed\n",
    "                if not data.empty:\n",
    "                    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                    # Step 4: Assign column names (if there are more columns)\n",
    "                    data.columns = ['col' + str(i) for i in range(data.shape[1])]\n",
    "\n",
    "                    # Step 5: Convert columns to appropriate types (e.g., numeric)\n",
    "                    data.iloc[:, 1:] = data.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "                    # Step 6: Handle missing values (example: fill NaN with the mean of the column)\n",
    "                    data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "                    # Append the processed data to the final DataFrame\n",
    "                    processed_data = pd.concat([processed_data, data], ignore_index=True)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {d}: {e}\")\n",
    "\n",
    "# Display the cleaned data\n",
    "print(processed_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmYXiHGKMAHt"
   },
   "outputs": [],
   "source": [
    "\n",
    "      \n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"/content/WESAD/\"\n",
    "files = os.listdir(base_path)\n",
    "data = {}\n",
    "\n",
    "for file in files:\n",
    "    if file.endswith('.pdf'):\n",
    "        continue\n",
    "    for d in os.listdir(os.path.join(base_path, file)):\n",
    "        if d.endswith('.csv'):\n",
    "            file_path = os.path.join(base_path, file, d)\n",
    "            temp_data = pd.read_csv(file_path, skiprows=2)\n",
    "            temp_data.dropna(axis=1, how='all', inplace=True)\n",
    "            numeric_data = temp_data.select_dtypes(include=['number'])\n",
    "            temp_data[numeric_data.columns] = numeric_data.fillna(numeric_data.mean())\n",
    "            data[d] = temp_data\n",
    "            print(temp_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPDJ0vecE44I"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "  import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_data(data):\n",
    "    if '# ORDER' not in data.columns:\n",
    "        raise KeyError(\"The '# ORDER' column is missing in the dataset.\")\n",
    "    features = data.iloc[:, 1:].select_dtypes(include=['number']).values\n",
    "    labels = data['# ORDER'].values\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    features = pad_sequences(features, padding='post', dtype='float32')\n",
    "    return features, labels\n",
    "\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "for file_name, temp_data in data.items():\n",
    "    try:\n",
    "        X, y = preprocess_data(temp_data)\n",
    "        X_data.append(X)\n",
    "        y_data.append(y)\n",
    "    except KeyError as e:\n",
    "        print(f\"Skipping file '{file_name}': {e}\")\n",
    "\n",
    "if X_data and y_data:\n",
    "    X_data = np.concatenate(X_data, axis=0)\n",
    "    y_data = np.concatenate(y_data, axis=0)\n",
    "    print(\"Prepared data shapes:\", X_data.shape, y_data.shape)\n",
    "else:\n",
    "    print(\"No valid data was processed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptoZpk6RGd1O"
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_data)\n",
    "y_one_hot = to_categorical(y_encoded)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0.0, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    SimpleRNN(128, activation='relu', return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    SimpleRNN(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "model.save('rnn_model.h5')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XuNdHszriLR"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_data = data[['ACC', 'EDA', 'TEMP']]  \n",
    "y_data = data['label']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWhgg5W4BGtE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_train_rnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test_rnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "\n",
    "def create_rnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(64, input_shape=input_shape, activation='tanh', return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(SimpleRNN(32, activation='tanh'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (X_train_rnn.shape[1], X_train_rnn.shape[2])\n",
    "model = create_rnn_model(input_shape)\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "history = model.fit(X_train_rnn, y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(X_test_rnn, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test_rnn)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "98WevQExBHey"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "scratchpad",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
