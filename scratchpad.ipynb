{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rachit180/FER/blob/main/scratchpad.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIYdn1woOS1n",
        "outputId": "fdfa7e45-7cbb-4035-aec3-a72d4231b597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        " from google.colab import drive\n",
        " drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xqPPfyaqvwBE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/WESAD (1).zip\"\n"
      ],
      "metadata": {
        "id": "qfIZB4R1vxwm",
        "outputId": "57988039-95c1-48df-e306-9e74a74cf35e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace WESAD/S10/S10.pkl? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax,Nadam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization, MaxPooling2D, Activation, Input, ZeroPadding2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.regularizers import l1, l2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "AAkheiOl9j-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "files = os.listdir(\n",
        "    '/content/WESAD/'\n",
        ")\n",
        "for file in files:\n",
        "  if file.endswith('.pdf') : continue\n",
        "  for d in os.listdir(f'/content/WESAD/{file}'):\n",
        "    if d.endswith('.csv'):\n",
        "      data = pd.read_csv(f'/content/WESAD/{file}/{d}')\n",
        "      break\n",
        ""
      ],
      "metadata": {
        "id": "ZuPjDXV7-gN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "aopioOZmGVGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Base directory containing subject folders\n",
        "base_path = '/content/WESAD/'\n",
        "\n",
        "# Initialize lists for features and labels\n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "# Iterate through all subject folders\n",
        "subject_folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f))]\n",
        "\n",
        "for subject in subject_folders:\n",
        "    try:\n",
        "        subject_path = os.path.join(base_path, subject)\n",
        "\n",
        "        # Load synchronized data and labels from .pkl file with specified encoding\n",
        "        with open(os.path.join(subject_path, f\"{subject}.pkl\"), 'rb') as f:\n",
        "            data = pickle.load(f, encoding='latin1')  # Change encoding if necessary\n",
        "\n",
        "        # Extract chest sensor data\n",
        "        chest_acc = data['signal']['chest']['ACC']  # Accelerometer (x, y, z)\n",
        "        chest_eda = data['signal']['chest']['EDA']  # Electrodermal activity\n",
        "        chest_temp = data['signal']['chest']['TEMP']  # Temperature\n",
        "\n",
        "        # Extract wrist sensor data\n",
        "        wrist_acc = data['signal']['wrist']['ACC']  # Accelerometer (x, y, z)\n",
        "        wrist_eda = data['signal']['wrist']['EDA']  # Electrodermal activity\n",
        "        wrist_temp = data['signal']['wrist']['TEMP']  # Temperature\n",
        "\n",
        "        # Combine all features from chest and wrist sensors\n",
        "        combined_features = np.hstack((\n",
        "            chest_acc, chest_eda[:, np.newaxis], chest_temp[:, np.newaxis],\n",
        "            wrist_acc, wrist_eda[:, np.newaxis], wrist_temp[:, np.newaxis]\n",
        "        ))\n",
        "\n",
        "        # Standardize features\n",
        "        scaler = StandardScaler()\n",
        "        standardized_features = scaler.fit_transform(combined_features)\n",
        "\n",
        "        # Extract labels\n",
        "        labels = data['label']  # Stress levels: 0=undefined, 1=baseline, 2=stress, etc.\n",
        "\n",
        "        # Append to main dataset\n",
        "        X_data.append(standardized_features)\n",
        "        y_data.append(labels)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing subject {subject}: {e}\")\n",
        "\n",
        "# Ensure there's valid data to process\n",
        "if X_data and y_data:\n",
        "    # Concatenate data from all subjects\n",
        "    X_data = np.concatenate(X_data, axis=0)\n",
        "    y_data = np.concatenate(y_data, axis=0)\n",
        "\n",
        "    # Pad sequences for RNN models\n",
        "    X_data_padded = pad_sequences(X_data, padding='post', dtype='float32')\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_data_padded, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
        "\n",
        "    # Display data shapes\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "    print(\"X_test shape:\", X_test.shape)\n",
        "    print(\"y_test shape:\", y_test.shape)\n",
        "else:\n",
        "    print(\"No valid data was processed.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Whz_LxC3nR2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/WESAD/S15/S15_quest.csv'\n",
        "\n",
        "# Read the file with appropriate delimiter\n",
        "data = pd.read_csv(file_path, header=None, delimiter=';')\n",
        "\n",
        "# Remove metadata rows (e.g., rows starting with # or empty rows)\n",
        "data = data[~data[0].astype(str).str.startswith('#')]\n",
        "data = data.dropna(how='all')  # Drop completely empty rows\n",
        "\n",
        "# Reset index after filtering\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Debugging: Check the dataset structure\n",
        "print(\"Cleaned Data Sample:\")\n",
        "print(data.head())\n",
        "\n",
        "# Extract sections relevant for features (X) and labels (Y)\n",
        "sections = {\n",
        "    'PANAS': data[data[0].str.contains('PANAS', na=False)].iloc[:, 1:].astype(float).values,\n",
        "    'STAI': data[data[0].str.contains('STAI', na=False)].iloc[:, 1:].astype(float).values,\n",
        "    'DIM': data[data[0].str.contains('DIM', na=False)].iloc[:, 1:].astype(float).values,\n",
        "    'SSSQ': data[data[0].str.contains('SSSQ', na=False)].iloc[:, 1:].astype(float).values,\n",
        "}\n",
        "\n",
        "# Combine features (X) from all sections\n",
        "X_data = np.hstack([sections['PANAS'], sections['STAI'], sections['DIM'], sections['SSSQ']])\n",
        "\n",
        "# Create labels (Y)\n",
        "# Example: Use `SSSQ` as stress levels or assign custom labels based on rows\n",
        "# For simplicity, we label high values in SSSQ (e.g., mean > 3) as stress\n",
        "SSSQ_mean = np.mean(sections['SSSQ'], axis=1)\n",
        "y_data = np.where(SSSQ_mean > 3, 1, 0)  # Binary labels: 1 = Stress, 0 = No Stress\n",
        "\n",
        "# Debugging: Check shapes\n",
        "print(f\"Feature shape: {X_data.shape}, Label shape: {y_data.shape}\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_data = scaler.fit_transform(X_data)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
        "\n",
        "# Debugging: Display data splits\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ],
      "metadata": {
        "id": "4642DrmYpbZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Ensure the data is in the correct shape for RNN\n",
        "# RNN expects data in the format (samples, timesteps, features)\n",
        "# Assume each row is treated as one timestep for simplicity\n",
        "X_train_rnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_test_rnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "# Define the RNN model\n",
        "def create_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(64, input_shape=input_shape, activation='tanh', return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(SimpleRNN(32, activation='tanh'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification, use sigmoid activation\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "input_shape = (X_train_rnn.shape[1], X_train_rnn.shape[2])\n",
        "model = create_rnn_model(input_shape)\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_rnn, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_rnn, y_test, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_rnn)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Generate classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "lbyBhxWtpkVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Ensure the data is in the correct shape for RNN\n",
        "# RNN expects data in the format (samples, timesteps, features)\n",
        "# Assume each row is treated as one timestep for simplicity\n",
        "X_train_rnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_test_rnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "# Define the RNN model\n",
        "def create_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(64, input_shape=input_shape, activation='tanh', return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(SimpleRNN(32, activation='tanh'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification, use sigmoid activation\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "input_shape = (X_train_rnn.shape[1], X_train_rnn.shape[2])\n",
        "model = create_rnn_model(input_shape)\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_rnn, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_rnn, y_test, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_rnn)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Generate classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "NsSQ9dRlouRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty DataFrame for storing the processed data\n",
        "processed_data = pd.DataFrame()\n",
        "\n",
        "# Loop through the files in the directory\n",
        "files = os.listdir('/content/WESAD/')\n",
        "\n",
        "for file in files:\n",
        "    if file.endswith('.pdf'):  # Skip any .pdf files\n",
        "        continue\n",
        "\n",
        "    for d in os.listdir(f'/content/WESAD/{file}'):\n",
        "        if d.endswith('.csv'):  # Process only .csv files\n",
        "            # Read the CSV file with explicit delimiter ';'\n",
        "            try:\n",
        "                data = pd.read_csv(f'/content/WESAD/{file}/{d}', header=None, delimiter=';')\n",
        "\n",
        "                # Debug: Check the first few rows to ensure data is loaded correctly\n",
        "                print(f\"Loaded {d} from {file}\")\n",
        "                print(data.head())  # Display the first few rows of the file\n",
        "\n",
        "                # Step 1: Remove rows that are just separators (e.g., rows with only ';')\n",
        "                data = data[~data.apply(lambda row: row.astype(str).str.contains(';;;;', regex=False).all(), axis=1)]\n",
        "\n",
        "                # Step 2: Remove rows that start with '#', as they are metadata or comments\n",
        "                data = data[~data[0].str.startswith('#', na=False)]\n",
        "\n",
        "                # Step 3: Reset the columns if any rows are removed\n",
        "                if not data.empty:\n",
        "                    data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "                    # Step 4: Assign column names (if there are more columns)\n",
        "                    data.columns = ['col' + str(i) for i in range(data.shape[1])]\n",
        "\n",
        "                    # Step 5: Convert columns to appropriate types (e.g., numeric)\n",
        "                    data.iloc[:, 1:] = data.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "                    # Step 6: Handle missing values (example: fill NaN with the mean of the column)\n",
        "                    data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "                    # Append the processed data to the final DataFrame\n",
        "                    processed_data = pd.concat([processed_data, data], ignore_index=True)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file {d}: {e}\")\n",
        "\n",
        "# Display the cleaned data\n",
        "print(processed_data.head())\n"
      ],
      "metadata": {
        "id": "VGV_GLpdKU63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the root directory containing the subdirectories\n",
        "base_path = \"/content/WESAD/\"\n",
        "\n",
        "# Get a list of all files and directories in the base path\n",
        "files = os.listdir(base_path)\n",
        "\n",
        "# Initialize a dictionary to store the data\n",
        "data = {}\n",
        "\n",
        "# Iterate through each file/subdirectory\n",
        "for file in files:\n",
        "    # Skip files that are PDFs\n",
        "    if file.endswith('.pdf'):\n",
        "        continue\n",
        "\n",
        "    # Iterate through the contents of the subdirectories\n",
        "    for d in os.listdir(os.path.join(base_path, file)):\n",
        "        # If the file is a CSV\n",
        "        if d.endswith('.csv'):\n",
        "            file_path = os.path.join(base_path, file, d)\n",
        "            print(f\"Loading {d} from {file_path}\")\n",
        "\n",
        "            # Read the CSV file, skipping metadata rows (first 2 rows in this case)\n",
        "            temp_data = pd.read_csv(file_path, skiprows=2)\n",
        "\n",
        "            # Drop any unnecessary columns (like those that only contain NaN values)\n",
        "            temp_data.dropna(axis=1, how='all', inplace=True)\n",
        "\n",
        "            # Select only numeric columns for mean imputation\n",
        "            numeric_data = temp_data.select_dtypes(include=['number'])\n",
        "\n",
        "            # Fill NaN values with column means in numeric columns\n",
        "            temp_data[numeric_data.columns] = numeric_data.fillna(numeric_data.mean())\n",
        "\n",
        "            # Store the cleaned data in the dictionary\n",
        "            data[d] = temp_data\n",
        "\n",
        "            # Output first 5 rows to confirm the data\n",
        "            print(temp_data.head())\n",
        "\n",
        "# After cleaning, you can access the cleaned data like this:\n",
        "# data[\"S17_quest.csv\"], data[\"S16_quest.csv\"], etc.\n"
      ],
      "metadata": {
        "id": "nmYXiHGKMAHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Example preprocessing function for one file\n",
        "def preprocess_data(data):\n",
        "    # Ensure the `# ORDER` column exists\n",
        "    if '# ORDER' not in data.columns:\n",
        "        raise KeyError(\"The '# ORDER' column is missing in the dataset.\")\n",
        "\n",
        "    # Select relevant numeric columns as features (all except the first column)\n",
        "    features = data.iloc[:, 1:].select_dtypes(include=['number']).values  # Numeric columns only\n",
        "    labels = data['# ORDER'].values  # Use the `# ORDER` column as labels\n",
        "\n",
        "    # Standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    features = scaler.fit_transform(features)\n",
        "\n",
        "    # Pad sequences (if your data contains varying sequence lengths)\n",
        "    features = pad_sequences(features, padding='post', dtype='float32')\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Prepare the data (assuming 'data' dictionary is already defined)\n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for file_name, temp_data in data.items():\n",
        "    try:\n",
        "        X, y = preprocess_data(temp_data)\n",
        "        X_data.append(X)\n",
        "        y_data.append(y)\n",
        "    except KeyError as e:\n",
        "        print(f\"Skipping file '{file_name}': {e}\")\n",
        "\n",
        "# Convert to numpy arrays (if any valid data was processed)\n",
        "if X_data and y_data:\n",
        "    X_data = np.concatenate(X_data, axis=0)\n",
        "    y_data = np.concatenate(y_data, axis=0)\n",
        "\n",
        "    # Check the shape of the prepared data\n",
        "    print(\"Prepared data shapes:\", X_data.shape, y_data.shape)\n",
        "else:\n",
        "    print(\"No valid data was processed.\")\n"
      ],
      "metadata": {
        "id": "dPDJ0vecE44I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Encode labels into integers and one-hot encode\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_data)\n",
        "y_one_hot = to_categorical(y_encoded)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the RNN model\n",
        "model = Sequential([\n",
        "    tf.keras.layers.Masking(mask_value=0.0, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    SimpleRNN(128, activation='relu', return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    SimpleRNN(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(y_train.shape[1], activation='softmax')  # Output layer with softmax for multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Save the model for future use\n",
        "model.save('rnn_model.h5')\n",
        "\n",
        "# Plot training history\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ptoZpk6RGd1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the dataset has columns 'ACC', 'EDA', 'TEMP', and the target column is 'label'\n",
        "# Example: You have a dataframe `data` with these columns\n",
        "# Split the features and target\n",
        "\n",
        "X_data = data[['ACC', 'EDA', 'TEMP']]  # Select feature columns\n",
        "y_data = data['label']  # Select the target column (e.g., 'label')\n"
      ],
      "metadata": {
        "id": "0XuNdHszriLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Ensure the data is in the correct shape for RNN\n",
        "# RNN expects data in the format (samples, timesteps, features)\n",
        "# Assume each row is treated as one timestep for simplicity\n",
        "X_train_rnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_test_rnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "# Define the RNN model\n",
        "def create_rnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(64, input_shape=input_shape, activation='tanh', return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(SimpleRNN(32, activation='tanh'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification, use sigmoid activation\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Initialize the model\n",
        "input_shape = (X_train_rnn.shape[1], X_train_rnn.shape[2])\n",
        "model = create_rnn_model(input_shape)\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_rnn, y_train,\n",
        "                    validation_split=0.2,\n",
        "                    epochs=20,\n",
        "                    batch_size=32,\n",
        "                    verbose=1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_rnn, y_test, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_rnn)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int).flatten()\n",
        "\n",
        "# Generate classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes))\n"
      ],
      "metadata": {
        "id": "ZWhgg5W4BGtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
      ],
      "metadata": {
        "id": "98WevQExBHey"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}